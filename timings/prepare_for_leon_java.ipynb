{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert code to run in Leon French's java app\n",
    "\n",
    "Leon French shared a java optimizer with us March 5, 2019 that is probably faster than our python code, but probably is not re-ranking probes the same way we are. This code will load selected dataframes, align their wellids and probes, and save them out as tsvs so Leon's optimizer can read them.\n",
    "\n",
    "Spiro got instructions from Leon via email January 30, 2020 to run the optimizer and see if we can save some time running our optimizations. Those instructions informed the format of tsv files generated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define selected files to convert. \"\"\"\n",
    "\n",
    "# This is an expression matrix unsplit after being SRS-adjusted.\n",
    "# It has 58,692 probes and 3,704 wellids.\n",
    "# It needs to be pruned down to 15,745 probes (The fornito selection) and 1280 wellids (all of the 1280 mappable to glasser parcels).\n",
    "# Stored as a python pickle, it's 1.7GB.\n",
    "expr_file_whole = \"/data/cache/fornito-glasser-expression-srs.df\"\n",
    "\n",
    "# This is an expression matrix split into training set #202 after being SRS-adjusted.\n",
    "# It has 15,745 probes (The fornito selection) and 640 wellids (half of the 1280 mappable to glasser parcels).\n",
    "# Stored as a python pickle, it's 78MB.\n",
    "expr_file_half = \"/data/splits/sub-all_hem-A_samp-glasser_prob-fornito/batch-train00202/parcelby-wellid_splitby-wellid.srs.df\"\n",
    "\n",
    "# This is an expression matrix split into training set #202 after being SRS-adjusted.\n",
    "# It has 15,745 probes (The fornito selection) and 640 wellids (half of the 1280 mappable to glasser parcels).\n",
    "# Stored as a python pickle, it's 39MB.\n",
    "expr_file_quarter = \"/data/splits/sub-all_hem-A_samp-glasser_prob-fornito/batch-train00402/parcelby-wellid_splitby-wellid.srs.df\"\n",
    "\n",
    "# This is a connectivity-similarity matrix.\n",
    "# It has 2,731 wellids on each axis, so most of those can be dropped to match the wellids in expression.\n",
    "# Stored as a python pickle, it's 57MB.\n",
    "connsim_file = \"/data/conn/hcp_niftismooth_grandmean_sim.df\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1,139 wellids common to both 1,280 in expression whole and 2,731 in connectivity similarity.\n",
      "Found 568 wellids common to both 640 in expression split-half and 2,731 in connectivity similarity.\n",
      "Found 283 wellids common to both 320 in expression split-quarter and 2,731 in connectivity similarity.\n",
      "Whole expression went from (15745, 1280) to (15745, 1139) and connectivity-similarity from (2731, 2731) to (1139, 1139).\n",
      "Split-half expression went from (15745, 640) to (15745, 568) and connectivity-similarity from (2731, 2731) to (568, 568).\n",
      "Split quarter expression went from (15745, 320) to (15745, 283) and connectivity-similarity from (2731, 2731) to (283, 283).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load the files and remove any non-comparable wellids. \"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "df_expr_whole = pickle.load(open(expr_file_whole, \"rb\"))\n",
    "df_expr_half = pickle.load(open(expr_file_half, \"rb\"))\n",
    "df_expr_quarter = pickle.load(open(expr_file_quarter, \"rb\"))\n",
    "df_connsim = pickle.load(open(connsim_file, \"rb\"))\n",
    "\n",
    "# Figure out which wellids are common to both dataframes, in original df_expr order\n",
    "wellids_whole = [id for id in df_expr_whole.columns if id in df_connsim.columns]\n",
    "print(\"Found {:,} wellids common to both {:,} in expression whole and {:,} in connectivity similarity.\".format(\n",
    "    len(wellids_whole), len(df_expr_whole.columns), len(df_connsim.columns),\n",
    "))\n",
    "\n",
    "wellids_half = [id for id in df_expr_half.columns if id in df_connsim.columns]\n",
    "print(\"Found {:,} wellids common to both {:,} in expression split-half and {:,} in connectivity similarity.\".format(\n",
    "    len(wellids_half), len(df_expr_half.columns), len(df_connsim.columns),\n",
    "))\n",
    "\n",
    "wellids_quarter = [id for id in df_expr_quarter.columns if id in df_connsim.columns]\n",
    "print(\"Found {:,} wellids common to both {:,} in expression split-quarter and {:,} in connectivity similarity.\".format(\n",
    "    len(wellids_quarter), len(df_expr_quarter.columns), len(df_connsim.columns),\n",
    "))\n",
    "\n",
    "# Keep only the relevant (common to both) data.\n",
    "df_expr_whole_pruned = df_expr_whole.loc[:, wellids_whole]\n",
    "df_connsim_whole_pruned = df_connsim.loc[wellids_whole, wellids_whole]\n",
    "print(\"Whole expression went from {} to {} and connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_whole.shape, df_expr_whole_pruned.shape, df_connsim.shape, df_connsim_whole_pruned.shape\n",
    "))\n",
    "\n",
    "df_expr_half_pruned = df_expr_half.loc[:, wellids_half]\n",
    "df_connsim_half_pruned = df_connsim.loc[wellids_half, wellids_half]\n",
    "print(\"Split-half expression went from {} to {} and connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_half.shape, df_expr_half_pruned.shape, df_connsim.shape, df_connsim_half_pruned.shape\n",
    "))\n",
    "\n",
    "df_expr_quarter_pruned = df_expr_quarter.loc[:, wellids_quarter]\n",
    "df_connsim_quarter_pruned = df_connsim.loc[wellids_quarter, wellids_quarter]\n",
    "print(\"Split quarter expression went from {} to {} and connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_quarter.shape, df_expr_quarter_pruned.shape, df_connsim.shape, df_connsim_quarter_pruned.shape\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expr_whole.to_pickle(\"./ahba_expr_whole.df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Write the matched set to java-friendly tsvs. \"\"\"\n",
    "\n",
    "# This file ends up 342.5MB (the entire AHBA matrix was over 3GB)\n",
    "df_expr_whole_pruned.index.name = \"\"\n",
    "df_expr_whole_pruned.to_csv(\"./ahba_expr_whole.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_connsim_whole_pruned.index.name = \"\"\n",
    "df_connsim_whole_pruned.to_csv(\"./hcp_conn_sim_whole.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_expr_half_pruned.index.name = \"\"\n",
    "df_expr_half_pruned.to_csv(\"./ahba_expr_train202.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_connsim_half_pruned.index.name = \"\"\n",
    "df_connsim_half_pruned.to_csv(\"./hcp_conn_sim_train202.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_expr_quarter_pruned.index.name = \"\"\n",
    "df_expr_quarter_pruned.to_csv(\"./ahba_expr_train402.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_connsim_quarter_pruned.index.name = \"\"\n",
    "df_connsim_quarter_pruned.to_csv(\"./hcp_conn_sim_train402.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expr_quarter_pruned.to_pickle(\"./ahba_expr_train402.df\")\n",
    "df_connsim_quarter_pruned.to_pickle(\"./hcp_conn_sim_train402.df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## For parcellated data, run separately\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full unparcellated dataframe [15,745 x 1,280] reduced to [15,745 x [177].\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load unparcellated data, then parcellate it. \"\"\"\n",
    "\n",
    "from pygest.rawdata.glasser import glasser_parcel_map\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def average_expr_per_parcel(wellid_expression, parcel_map):\n",
    "    \"\"\" Average expression values over all wellids in each parcel_map-defined parcel.\n",
    "        (copied from pygest split.run and modified)\n",
    "    \"\"\"\n",
    "\n",
    "    parcels = pd.DataFrame(\n",
    "        data={'parcel': [parcel_map[x] for x in wellid_expression.columns]},\n",
    "        index=wellid_expression.columns\n",
    "    )\n",
    "    parcel_means = {}\n",
    "    for parcel in sorted(list(set(parcel_map.values()))):\n",
    "        parcel_idx = parcels[parcels['parcel'] == parcel].index\n",
    "        if len(parcel_idx) > 0:\n",
    "            parcel_means[parcel] = wellid_expression.loc[:, parcel_idx].mean(axis=1)\n",
    "        else:\n",
    "            print(\"  parcel {} had no samples\".format(parcel))\n",
    "    return pd.DataFrame(data=parcel_means)\n",
    "\n",
    "# SRS-adjusted expression is already loaded, by wellid, as df_expr_whole, from expr_file_whole\n",
    "df_expr_parcellated = average_expr_per_parcel(df_expr_whole, glasser_parcel_map)\n",
    "\n",
    "print(\"Full unparcellated dataframe [{:,} x {:,}] reduced to [{:,} x [{:,}].\".format(\n",
    "    df_expr_whole.shape[0], df_expr_whole.shape[1], df_expr_parcellated.shape[0], df_expr_parcellated.shape[1],\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 parcels are in both expression and connectivity-similarity.\n",
      "Parcellated expression went from (15745, 177) to (15745, 177) and parcellated connectivity-similarity from (360, 360) to (177, 177).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Find agreement between parcellated expresison and connectivity, then save out matching pair. \"\"\"\n",
    "\n",
    "# This is a parcellated connectivity-similarity matrix.\n",
    "# It has 360 parcels on each axis, so half of those can be dropped to match the left hemisphere wellids in expression.\n",
    "# Stored as a python pickle, it's 1MB.\n",
    "parcellated_connsim_file = \"/data/conn/glasser-connectivity_sim.df\"\n",
    "\n",
    "df_connsim_parcellated = pickle.load(open(parcellated_connsim_file, \"rb\"))\n",
    "\n",
    "matching_parcels = [id for id in df_expr_parcellated.columns if id in df_connsim_parcellated.columns]\n",
    "print(\"{:,} parcels are in both expression and connectivity-similarity.\".format(len(matching_parcels)))\n",
    "\n",
    "df_expr_parcellated_pruned = df_expr_parcellated[matching_parcels]\n",
    "df_connsim_parcellated_pruned = df_connsim_parcellated.reindex(matching_parcels)[matching_parcels]\n",
    "print(\"Parcellated expression went from {} to {} and parcellated connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_parcellated.shape, df_expr_parcellated_pruned.shape, df_connsim_parcellated.shape, df_connsim_parcellated_pruned.shape\n",
    "))\n",
    "\n",
    "df_expr_parcellated_pruned.to_csv(\"./ahba_expr_whole_parby-glasser.tsv\", sep=\"\\t\")\n",
    "df_expr_parcellated_pruned.to_pickle(\"./ahba_expr_whole_parby-glasser.df\")\n",
    "df_connsim_parcellated_pruned.to_csv(\"./hcp_conn_sim_whole_parby-glasser.tsv\", sep=\"\\t\")\n",
    "df_connsim_parcellated_pruned.to_pickle(\"./hcp_conn_sim_whole_parby-glasser.df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schmidt_geconn",
   "language": "python",
   "name": "schmidt_geconn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
