{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a few gene lists\n",
    "\n",
    "There are a couple of ways to assess the similarity of gene lists:\n",
    "\n",
    "- If most of the genes in the entire list are relevant, it makes sense to compare one ranked list of genes with another. A Kendall tau correlation can do that.\n",
    "\n",
    "- If only a few of the genes in a list are likely to be relevant, the noise of the many will drown out the signal of the few. In this case, it is probably best to determine how likely the few important genes in one list are to be important in the other. Background genes would be ignored in this case.\n",
    "\n",
    "Here, we generate a few lists to support those comparisons. Our main results are comparisons between HCP connectivity data and AHBA gene expression data. We generate a list for each distance mask used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define some lists for looping. \"\"\"\n",
    "masks = ['00', '16', '32', '64', ]\n",
    "shuffles = [\"none\", \"agno\", \"dist\", \"edge\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section determines the threshold for how many ranked genes to use in ermineJ.\n",
    "\n",
    "The average peak during training leaves about 70 genes beyond the peak. So, for thresholding the ranked gene lists, taking the top 70 will be about the same as taking the genes surviving beyond the peak. When averaging rankings across split-halves, each split-half run will peak at a different place. So there is no way to have an actual specific peak in averaged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak for 00 at 72 +/- 18\n",
      "     00 agno at 88 +/- 25\n",
      "     00 dist at 79 +/- 24\n",
      "     00 edge at 45 +/- 12\n",
      "Peak for 16 at 72 +/- 14\n",
      "     16 agno at 89 +/- 25\n",
      "     16 dist at 79 +/- 24\n",
      "     16 edge at 46 +/- 12\n",
      "Peak for 32 at 77 +/- 14\n",
      "     32 agno at 90 +/- 25\n",
      "     32 dist at 78 +/- 23\n",
      "     32 edge at 46 +/- 13\n",
      "Peak for 64 at 69 +/- 17\n",
      "     64 agno at 89 +/- 26\n",
      "     64 dist at 78 +/- 23\n",
      "     64 edge at 49 +/- 14\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load up the data, preprocessed by the ge_data_manager project.\n",
    "    We won't use this to generate rankings, but we can see quickly\n",
    "    where the peaks are in each run. \"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "pre_calc_results = {}\n",
    "for mask in masks:\n",
    "    with open(\"/data/plots/cache/hcpww{}speak_ol_post.df\".format(mask), \"br\") as f:\n",
    "        pre_calc_results[mask] = pickle.load(f)\n",
    "        derivatives = pre_calc_results[mask][pre_calc_results[mask]['shuffle'] == 'none']\n",
    "        print(\"Peak for {} at {:,} +/- {:0.0f}\".format(\n",
    "            mask,\n",
    "            15745 - int(derivatives['peak'].mean()),\n",
    "            derivatives['peak'].std(),\n",
    "        ))\n",
    "        for shuf in [\"agno\", \"dist\", \"edge\", ]:\n",
    "            shuffles = pre_calc_results[mask][pre_calc_results[mask]['shuffle'] == shuf]\n",
    "            print(\"     {} {} at {:,} +/- {:0.0f}\".format(\n",
    "                mask, shuf,\n",
    "                15745 - int(shuffles['peak'].mean()),\n",
    "                shuffles['peak'].std(),\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section recalculates rankings and p-values for a different cut-off\n",
    "\n",
    "Another way to determine which genes are \"relevant\" is to calculate a p-value for each gene's ranking vs the same gene's ranking in shuffled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Modified from ge_data_manager to include shuffled data,\n",
    "    the ranked_probes function iterates over result tsvs, ranking genes and saving them\n",
    "    into a single dataframe. \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pygest import algorithms\n",
    "from pygest.convenience import bids_val\n",
    "from pygest.rawdata import miscellaneous\n",
    "\n",
    "\n",
    "def name_from_path(path):\n",
    "    \"\"\" Extract critical pieces from the path to return an abbreviated column name.\n",
    "        Three items differ in each path: 4 shuffle-types, 16 splits, 16 seeds (plus None). \"\"\"\n",
    "    \n",
    "    shuf = bids_val(\"shuffle\", path)\n",
    "    if shuf == \"random\":\n",
    "        shuf = \"agno\" \n",
    "    if shuf == \"actual\":\n",
    "        shuf = \"real\" \n",
    "    \n",
    "    batch = bids_val(\"batch\", path)[-3:]\n",
    "    seed = bids_val(\"seed\", path)[-3:]\n",
    "    \n",
    "    return \"{}-{}-{}\".format(shuf, batch, seed)\n",
    "\n",
    "    \n",
    "def ranked_probes(tsvs, top):\n",
    "    \"\"\" Go through the files provided, at the threshold specified, and report probes in all files. \"\"\"\n",
    "\n",
    "    report_progress_on_items = range(int(len(tsvs)/10), len(tsvs), int(len(tsvs)/10))\n",
    "    all_rankings = pd.DataFrame()\n",
    "    for i, tsv in enumerate(tsvs):\n",
    "        df = pd.read_csv(tsv, sep='\\t')\n",
    "        rankings = pd.Series(data=df.index, index=df['probe_id'], name=name_from_path(tsv))\n",
    "        if i == 0:\n",
    "            all_rankings = pd.DataFrame(data=rankings)\n",
    "        else:\n",
    "            all_rankings[rankings.name] = rankings\n",
    "        # if i in report_progress_on_items:\n",
    "        #     print(\"Ranked {} of {} and counting...\".format(i, len(tsvs)))\n",
    "        if i == len(tsvs):\n",
    "            print(\"    ranked all probes in {} results.\".format(i + 1))\n",
    "    all_rankings['mean'] = all_rankings.mean(axis=1)\n",
    "    all_rankings['entrez_id'] = all_rankings.index.map(miscellaneous.map_pid_to_eid_fornito)\n",
    "    return all_rankings.sort_values('mean', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Rank probes/genes for each and every run. \"\"\"\n",
    "\n",
    "rankings = {}\n",
    "for mask in masks:\n",
    "    tsv_files = pre_calc_results[mask]['path']\n",
    "    rankings[mask] = ranked_probes(tsv_files, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Determine, for each probe/gene, how likely it is for a real ranking to be higher than a shuffled ranking. \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def p_probe(probe_id, df):\n",
    "    \"\"\" Return probability (p-value) a real rank is higher than shuffled. \"\"\"\n",
    "    \n",
    "    ps = {}\n",
    "    real_runs = [c for c in df.columns if \"real\" in c]\n",
    "    for shuf in [\"agno\", \"dist\", \"edge\", ]:\n",
    "        shuffled_runs = [c for c in df.columns if shuf in c]\n",
    "\n",
    "        n_better = 0\n",
    "        n_worse = 0\n",
    "        n_total = 0\n",
    "        for real in df.loc[probe_id, real_runs]:\n",
    "            for baseline in df.loc[probe_id, shuffled_runs]:\n",
    "                n_total += 1\n",
    "                if real < baseline:\n",
    "                    n_better += 1\n",
    "                else:\n",
    "                    n_worse += 1\n",
    "            # print(\"    {} vs {} of {}\".format(n_better, n_worse, n_total))\n",
    "        ps[shuf] = n_worse / n_total\n",
    "\n",
    "        # print(\"Mean real rank = {:,} vs mean {} rank of {:,}; {:,} better, {:,} worse, out of {:,}; p = {:0.3f}\".format(\n",
    "        #     int(np.mean(df.loc[probe_id, real_runs])), shuf, int(np.mean(df.loc[probe_id, shuffled_runs])),\n",
    "        #     n_better, n_worse, n_total, ps[shuf]\n",
    "        # ))\n",
    "        \n",
    "    return ps[\"agno\"], ps[\"dist\"], ps[\"edge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating p-values for 00mm-masked results.\n",
      "Calculating p-values for 16mm-masked results.\n",
      "Calculating p-values for 32mm-masked results.\n",
      "Calculating p-values for 64mm-masked results.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Calculate p-values for real vs each of three shuffle types. \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_p_values = {}\n",
    "for mask in masks:\n",
    "    p_values = {}\n",
    "    print(\"Calculating p-values for {}mm-masked results.\".format(mask))\n",
    "    for pid in rankings[mask].index:\n",
    "        p_a, p_d, p_e = p_probe(pid, rankings[mask])\n",
    "        p_values[pid] = {\n",
    "            'entrez_id': int(rankings[mask].loc[pid, 'entrez_id']),\n",
    "            'agno': p_a,\n",
    "            'dist': p_d,\n",
    "            'edge': p_e,\n",
    "        }\n",
    "    df_p_values[mask] = pd.DataFrame(data=p_values).T\n",
    "    df_p_values[mask]['entrez_id'] = df_p_values[mask]['entrez_id'].astype(int)\n",
    "    # df_p_values[mask] = df_p_values[mask].set_index('entrez_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define a kernel-density plot to assist in visualizing the distributions of rankings. \"\"\"\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flatten = lambda l: [x for y in l for x in y]\n",
    "\n",
    "def plot_entrez_id(entrez_id, rank_data, p_data):\n",
    "    \"\"\" Plot ranking distributions for one entrez id. \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    real_list = flatten(rank_data[rank_data['entrez_id'] == entrez_id][[x for x in rank_data.columns if \"real\" in x]].values)\n",
    "    real_label = \"real\"\n",
    "    sns.distplot(real_list, ax=ax, kde_kws={\"color\": \"black\", \"label\": real_label}, hist_kws={\"color\": \"gray\"})\n",
    "    agno_list = flatten(rank_data[rank_data['entrez_id'] == entrez_id][[x for x in rank_data.columns if \"agno\" in x]].values)\n",
    "    agno_label = \"vs agno, p = {:0.5f}\".format(p_data.loc[entrez_id, 'agno'])\n",
    "    sns.distplot(agno_list, ax=ax, kde_kws={\"color\": \"green\", \"label\": agno_label}, hist_kws={\"color\": \"lightgreen\"})\n",
    "    dist_list = flatten(rank_data[rank_data['entrez_id'] == entrez_id][[x for x in rank_data.columns if \"dist\" in x]].values)\n",
    "    dist_label = \"vs dist, p = {:0.5f}\".format(p_data.loc[entrez_id, 'dist'])\n",
    "    sns.distplot(dist_list, ax=ax, kde_kws={\"color\": \"red\", \"label\": dist_label}, hist_kws={\"color\": \"mistyrose\"})\n",
    "    edge_list = flatten(rank_data[rank_data['entrez_id'] == entrez_id][[x for x in rank_data.columns if \"edge\" in x]].values)\n",
    "    edge_label = \"vs edge, p = {:0.5f}\".format(p_data.loc[entrez_id, 'edge'])\n",
    "    sns.distplot(edge_list, ax=ax, kde_kws={\"color\": \"magenta\", \"label\": edge_label}, hist_kws={\"color\": \"lavenderblush\"})\n",
    "    fig.suptitle(\"Entrez ID {}\".format(entrez_id))\n",
    "    return fig, ax\n",
    "\n",
    "# f, a = plot_entrez_id(57622, rankings[\"16\"], df_p_values[\"16\"].set_index('entrez_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 agno 331 probes with p < 0.05\n",
      "00 dist 11 probes with p < 0.05\n",
      "00 edge 2,441 probes with p < 0.05\n",
      "Mask 00 has 6 probes surviving all tests.\n",
      "    [9607, 9734, 10451, 10714, 57622, 80036]\n",
      "Mask 00 has 13112 probes failing all tests.\n",
      "    [1, 2, 12, 14, 16, 18, 19, 20, 21, 22, 23, 25]\n",
      "16 agno 333 probes with p < 0.05\n",
      "16 dist 10 probes with p < 0.05\n",
      "16 edge 2,404 probes with p < 0.05\n",
      "Mask 16 has 5 probes surviving all tests.\n",
      "    [9607, 9734, 10451, 57622, 80036]\n",
      "Mask 16 has 13140 probes failing all tests.\n",
      "    [1, 2, 12, 14, 16, 18, 19, 20, 21, 22, 23, 25]\n",
      "32 agno 331 probes with p < 0.05\n",
      "32 dist 12 probes with p < 0.05\n",
      "32 edge 2,319 probes with p < 0.05\n",
      "Mask 32 has 5 probes surviving all tests.\n",
      "    [9607, 9734, 57622, 80036, 171019]\n",
      "Mask 32 has 13228 probes failing all tests.\n",
      "    [1, 2, 12, 14, 16, 18, 19, 20, 21, 22, 23, 25]\n",
      "64 agno 333 probes with p < 0.05\n",
      "64 dist 9 probes with p < 0.05\n",
      "64 edge 1,954 probes with p < 0.05\n",
      "Mask 64 has 4 probes surviving all tests.\n",
      "    [9607, 9734, 57622, 80036]\n",
      "Mask 64 has 13597 probes failing all tests.\n",
      "    [1, 2, 12, 14, 16, 18, 19, 20, 21, 22, 23, 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Save out a csv file with entrez ids ordered by p-value for each mask and shuffle type. \"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "for mask in masks:\n",
    "    eids_all_good = set(df_p_values[mask]['entrez_id'])\n",
    "    eids_all_bad = set(df_p_values[mask]['entrez_id'])\n",
    "    for shuf in ['agno', 'dist', 'edge', ]:\n",
    "        df = df_p_values[mask][['entrez_id', shuf]].set_index('entrez_id').sort_values(shuf)\n",
    "        df = df.rename(columns={shuf: \"p\"})\n",
    "        print(\"{} {} {:,} probes with p < 0.05\".format(mask, shuf, (df['p'] < 0.05).sum()))\n",
    "        df.to_csv(\"./hcpww{}s_{}_p.csv\".format(mask, shuf))\n",
    "        \n",
    "        eids_all_good = eids_all_good.intersection(set(df['p'][df['p'] < 0.05].index))\n",
    "        eids_all_bad = eids_all_bad.intersection(set(df['p'][df['p'] >= 0.05].index))\n",
    "\n",
    "    print(\"Mask {} has {} probes surviving all tests.\".format(mask, len(eids_all_good)))\n",
    "    print(\"    {}\".format(sorted(list(eids_all_good))[:12]))\n",
    "    eid_to_plot = random.sample(eids_all_good, 1)[0]\n",
    "    f, a = plot_entrez_id(eid_to_plot, rankings[mask], df_p_values[mask].set_index('entrez_id'))\n",
    "    f.savefig(\"good_{}_{}.png\".format(mask, eid_to_plot))\n",
    "    f.clear()\n",
    "    \n",
    "    print(\"Mask {} has {} probes failing all tests.\".format(mask, len(eids_all_bad)))\n",
    "    print(\"    {}\".format(sorted(list(eids_all_bad))[:12]))\n",
    "    eid_to_plot = random.sample(eids_all_bad, 1)[0]\n",
    "    f, a = plot_entrez_id(eid_to_plot, rankings[mask], df_p_values[mask].set_index('entrez_id'))    \n",
    "    f.savefig(\"bad_{}_{}.png\".format(mask, eid_to_plot))\n",
    "    f.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schmidt_geconn",
   "language": "python",
   "name": "schmidt_geconn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
