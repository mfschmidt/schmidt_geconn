{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert code to run in Leon French's java app\n",
    "\n",
    "Leon French shared a java optimizer with us March 5, 2019 that is probably faster than our python code, but probably is not re-ranking probes the same way we are. This code will load selected dataframes, align their wellids and probes, and save them out as tsvs so Leon's optimizer can read them.\n",
    "\n",
    "Spiro got instructions from Leon via email January 30, 2020 to run the optimizer and see if we can save some time running our optimizations. Those instructions informed the format of tsv files generated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 14:05:09 [INFO] | PyGEST has initialized logging, and is running on host 'cardano'\n",
      "2020-03-25 14:05:09 [INFO] | Found 9 donors in /home/mike/ge_data/sourcedata/participants.tsv\n",
      "Whole expression matrix is [(15,745 rows of genes) x (1,280 columns of sample locations)].\n",
      "Whole expression matrix is [(15,745 rows of genes) x (177 columns of parcels)].\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Use pygest to generate SRS-adjusted expression matrix with only glasser-parcellable wellids.\n",
    "    Generate parcellated version of same data.\n",
    "    All other data sets have pre-built (for another project) dataframes on disk. Load them later.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pygest as ge\n",
    "from pygest.convenience import average_expr_per_parcel\n",
    "from pygest.rawdata.glasser import glasser_parcel_map\n",
    "\n",
    "\n",
    "data_dir = \"/home/mike/ge_data\"\n",
    "ge_data = ge.Data(data_dir)\n",
    "\n",
    "# This is an expression matrix SRS-adjusted and unsplit.\n",
    "# It has 15,745 probes (The fornito selection) and 1280 wellids (all of the 1280 mappable to glasser parcels).\n",
    "df_expr_whole = ge_data.expression(probes=\"fornito\", samples=\"glasser\", normalize=\"srs\")\n",
    "\n",
    "print(\"Whole expression matrix is [({:,} rows of genes) x ({:,} columns of sample locations)].\".format(\n",
    "    df_expr_whole.shape[0], df_expr_whole.shape[1]\n",
    "))\n",
    "\n",
    "df_expr_whole_parcellated = average_expr_per_parcel(df_expr_whole, glasser_parcel_map)\n",
    "\n",
    "print(\"Whole expression matrix is [({:,} rows of genes) x ({:,} columns of parcels)].\".format(\n",
    "    df_expr_whole_parcellated.shape[0], df_expr_whole_parcellated.shape[1]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define selected files, already prepared and saved. \"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# This is an expression matrix split into training set #202 after being SRS-adjusted.\n",
    "# It has 15,745 probes (The fornito selection) and 640 wellids (half of the 1280 mappable to glasser parcels).\n",
    "# Stored as a python pickle, it's 78MB.\n",
    "split_dir = \"splits/sub-all_hem-A_samp-glasser_prob-fornito/batch-train00202\"\n",
    "expr_file_half = os.path.join(data_dir, split_dir, \"parcelby-wellid_splitby-wellid.srs.df\")\n",
    "expr_file_half_parcellated = os.path.join(data_dir, split_dir, \"parcelby-glasser_splitby-wellid.srs.df\")\n",
    "\n",
    "# This is an expression matrix split into training set #202 after being SRS-adjusted.\n",
    "# It has 15,745 probes (The fornito selection) and 640 wellids (half of the 1280 mappable to glasser parcels).\n",
    "# Stored as a python pickle, it's 39MB.\n",
    "split_dir = \"splits/sub-all_hem-A_samp-glasser_prob-fornito/batch-train00402\"\n",
    "expr_file_quarter = os.path.join(data_dir, split_dir, \"parcelby-wellid_splitby-wellid.srs.df\")\n",
    "expr_file_quarter_parcellated = os.path.join(data_dir, split_dir, \"parcelby-glasser_splitby-wellid.srs.df\")\n",
    "\n",
    "# This is a connectivity matrix, generated from smoothed HCP data in matlab by Spiro\n",
    "# then cleaned, verified, and converted by Mike in build_conn_and_connsim_from_text/build_connectivity_matrices.ipynb\n",
    "conn_file = os.path.join(data_dir, \"conn/hcp_niftismooth_conn.df\")\n",
    "conn_file_parcellated = os.path.join(data_dir, \"conn/hcp_niftismooth_conn_parby-glasser.df\")\n",
    "\n",
    "# And this is a connectivity-similarity matrix, generated from the matrix just described.\n",
    "conn_sim_file = os.path.join(data_dir, \"conn/hcp_niftismooth_conn_sim.df\")\n",
    "conn_sim_file_parcellated = os.path.join(data_dir, \"conn/hcp_niftismooth_conn_parby-glasser_sim.df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load expression and connectivity-similarity data. \"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Already calculated: df_expr_whole, df_expr_whole_parcellated\n",
    "\n",
    "df_conn = pickle.load(open(conn_file, \"rb\"))\n",
    "df_conn_parcellated = pickle.load(open(conn_file_parcellated, \"rb\"))\n",
    "df_conn_sim = pickle.load(open(conn_sim_file, \"rb\"))\n",
    "df_conn_sim_parcellated = pickle.load(open(conn_sim_file_parcellated, \"rb\"))\n",
    "df_expr_half = pickle.load(open(expr_file_half, \"rb\"))\n",
    "df_expr_quarter = pickle.load(open(expr_file_quarter, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1,139 columns common to both 1,280 in expression and 2,731 in connectivity similarity.\n",
      "Found 568 columns common to both 640 in expression and 2,731 in connectivity similarity.\n",
      "Found 283 columns common to both 320 in expression and 2,731 in connectivity similarity.\n",
      "Found 176 columns common to both 177 in expression and 176 in connectivity similarity.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Determine locations without complete data across expression and connectivity. \"\"\"\n",
    "\n",
    "def overlapping_columns(dfc, dfe):\n",
    "    \"\"\" IN ORDER OF dfc, return list of columns (wellids or parcels) present in both dataframes. \"\"\"\n",
    "    \n",
    "    common_columns = [c for c in dfc.columns if c in dfe.columns]\n",
    "    print(\"Found {:,} columns common to both {:,} in expression and {:,} in connectivity similarity.\".format(\n",
    "        len(common_columns), len(dfe.columns), len(dfc.columns),\n",
    "    ))\n",
    "    \n",
    "    return common_columns\n",
    "\n",
    "    \n",
    "wellids_whole = overlapping_columns(df_conn_sim, df_expr_whole)\n",
    "wellids_half = overlapping_columns(df_conn_sim, df_expr_half)\n",
    "wellids_quarter = overlapping_columns(df_conn_sim, df_expr_quarter)\n",
    "\n",
    "parcels_whole = overlapping_columns(df_conn_sim_parcellated, df_expr_whole_parcellated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole expression went from (15745, 1280) to (15745, 1139) and connectivity-similarity from (2731, 2731) to (1139, 1139).\n",
      "Split-half expression went from (15745, 640) to (15745, 568) and connectivity-similarity from (2731, 2731) to (568, 568).\n",
      "Split quarter expression went from (15745, 320) to (15745, 283) and connectivity-similarity from (2731, 2731) to (283, 283).\n",
      "Whole expression went from (15745, 177) to (15745, 176) and connectivity-similarity from (176, 176) to (176, 176).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Exclude locations without complete data across expression and connectivity. \"\"\"\n",
    "\n",
    "# Keep only the relevant (common to both) data.\n",
    "df_expr_whole_pruned = df_expr_whole.loc[:, wellids_whole]\n",
    "df_conn_whole_pruned = df_conn.loc[wellids_whole, wellids_whole]\n",
    "df_conn_sim_whole_pruned = df_conn_sim.loc[wellids_whole, wellids_whole]\n",
    "print(\"Whole expression went from {} to {} and connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_whole.shape, df_expr_whole_pruned.shape, df_conn_sim.shape, df_conn_sim_whole_pruned.shape\n",
    "))\n",
    "\n",
    "df_expr_half_pruned = df_expr_half.loc[:, wellids_half]\n",
    "df_conn_half_pruned = df_conn.loc[wellids_half, wellids_half]\n",
    "df_conn_sim_half_pruned = df_conn_sim.loc[wellids_half, wellids_half]\n",
    "print(\"Split-half expression went from {} to {} and connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_half.shape, df_expr_half_pruned.shape, df_conn_sim.shape, df_conn_sim_half_pruned.shape\n",
    "))\n",
    "\n",
    "df_expr_quarter_pruned = df_expr_quarter.loc[:, wellids_quarter]\n",
    "df_conn_quarter_pruned = df_conn.loc[wellids_quarter, wellids_quarter]\n",
    "df_conn_sim_quarter_pruned = df_conn_sim.loc[wellids_quarter, wellids_quarter]\n",
    "print(\"Split quarter expression went from {} to {} and connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_quarter.shape, df_expr_quarter_pruned.shape, df_conn_sim.shape, df_conn_sim_quarter_pruned.shape\n",
    "))\n",
    "\n",
    "# Do the same for parcellated data.\n",
    "df_expr_whole_parcellated_pruned = df_expr_whole_parcellated.loc[:, parcels_whole]\n",
    "df_conn_whole_parcellated_pruned = df_conn_parcellated.loc[parcels_whole, parcels_whole]\n",
    "df_conn_sim_whole_parcellated_pruned = df_conn_sim_parcellated.loc[parcels_whole, parcels_whole]\n",
    "print(\"Whole expression went from {} to {} and connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_whole_parcellated.shape, df_expr_whole_parcellated_pruned.shape,\n",
    "    df_conn_sim_parcellated.shape, df_conn_sim_whole_parcellated_pruned.shape\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Write the matched set to java-friendly tsvs. \"\"\"\n",
    "\n",
    "data_save_path = \"./expr_and_conn_data\"\n",
    "\n",
    "def save_df_and_csv(df, name):\n",
    "    \"\"\" Save df as both tsv and df formats \"\"\"\n",
    "    df.index.name = \"\"\n",
    "    df.to_csv(os.path.join(data_save_path, name + \".tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(data_save_path, name + \".df\"))\n",
    "    \n",
    "\n",
    "save_df_and_csv(df_expr_whole_pruned, \"ahba_expr_whole\")\n",
    "save_df_and_csv(df_conn_whole_pruned, \"hcp_conn_whole\")\n",
    "save_df_and_csv(df_conn_sim_whole_pruned, \"hcp_conn_sim_whole\")\n",
    "save_df_and_csv(df_expr_half_pruned, \"ahba_expr_train202\")\n",
    "save_df_and_csv(df_conn_half_pruned, \"hcp_conn_train202\")\n",
    "save_df_and_csv(df_conn_sim_half_pruned, \"hcp_conn_sim_train202\")\n",
    "save_df_and_csv(df_expr_quarter_pruned, \"ahba_expr_train402\")\n",
    "save_df_and_csv(df_conn_quarter_pruned, \"hcp_conn_train402\")\n",
    "save_df_and_csv(df_conn_sim_quarter_pruned, \"hcp_conn_sim_train402\")\n",
    "save_df_and_csv(df_expr_whole_parcellated_pruned, \"ahba_expr_whole_parby-glasser\")\n",
    "save_df_and_csv(df_conn_whole_parcellated_pruned, \"hcp_conn_whole_parby-glasser\")\n",
    "save_df_and_csv(df_conn_sim_whole_parcellated_pruned, \"hcp_conn_whole_parby-glasser_sim\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Optionally, explore and clean up the data (This was already taken care of)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Check for bad columns and rows with np.nans \"\"\"\n",
    "\n",
    "def clean_conn_dataframe(conn):\n",
    "    bad_parcels = []\n",
    "    for col in conn.columns:\n",
    "        if sum(np.isnan(conn.loc[:, col].values)) > 1:\n",
    "            bad_parcels.append(col)\n",
    "    for row in conn.index:\n",
    "        if sum(np.isnan(conn.loc[row, :].values)) > 1:\n",
    "            bad_parcels.append(row)\n",
    "\n",
    "    print(\"Bad parcels: [{}]\".format(\", \".join(set(bad_parcels))))\n",
    "    good_columns = list(conn.columns)\n",
    "    for parcel in set(bad_parcels):\n",
    "        good_columns.remove(parcel)\n",
    "    \n",
    "    return conn.loc[good_columns, good_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad parcels: []\n",
      "[176 x 176] parcellated dataframe dropped 0 nans to become [176 x 176] with 0 nans.\n",
      "Bad parcels: []\n",
      "[176 x 176] parcellated dataframe dropped 0 nans to become [176 x 176] with 0 nans.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Clean some (already clean) dataframes.\n",
    "    If we had loaded bad data, or if this is used to handle future problematic data, it can be useful.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_conn_parcellated_cleaned = clean_conn_dataframe(df_conn_parcellated)\n",
    "print(\"[{} x {}] parcellated dataframe dropped {} nans to become [{} x {}] with {} nans.\".format(\n",
    "    df_conn_parcellated.shape[0], df_conn_parcellated.shape[1],\n",
    "    sum(sum(np.isnan(df_conn_parcellated.values))),\n",
    "    df_conn_parcellated_cleaned.shape[0], df_conn_parcellated_cleaned.shape[1],\n",
    "    sum(sum(np.isnan(df_conn_parcellated_cleaned.values))),\n",
    "))\n",
    "\n",
    "df_conn_sim_parcellated_cleaned = clean_conn_dataframe(df_conn_sim_parcellated)\n",
    "print(\"[{} x {}] parcellated dataframe dropped {} nans to become [{} x {}] with {} nans.\".format(\n",
    "    df_conn_sim_parcellated.shape[0], df_conn_sim_parcellated.shape[1],\n",
    "    sum(sum(np.isnan(df_conn_sim_parcellated.values))),\n",
    "    df_conn_sim_parcellated_cleaned.shape[0], df_conn_sim_parcellated_cleaned.shape[1],\n",
    "    sum(sum(np.isnan(df_conn_sim_parcellated_cleaned.values))),\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 parcels are in both expression and connectivity-similarity.\n",
      "Parcellated expression went from (15745, 176) to (15745, 176) and parcellated connectivity-similarity from (176, 176) to (176, 176).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Find agreement between parcellated expression and connectivity. \"\"\"\n",
    "\n",
    "matching_parcels = [id for id in df_connsim_parcellated_cleaned.columns if id in df_expr_whole_parcellated_pruned.columns]\n",
    "print(\"{:,} parcels are in both expression and connectivity-similarity.\".format(len(matching_parcels)))\n",
    "\n",
    "df_expr_whole_parcellated_cleaned = df_expr_whole_parcellated_pruned[matching_parcels]\n",
    "df_conn_parcellated_pruned = df_conn_parcellated_cleaned.loc[matching_parcels, matching_parcels]\n",
    "df_conn_sim_parcellated_pruned = df_conn_sim_parcellated_cleaned.loc[matching_parcels, matching_parcels]\n",
    "print(\"Parcellated expression went from {} to {} and parcellated connectivity-similarity from {} to {}.\".format(\n",
    "    df_expr_whole_parcellated_pruned.shape, df_expr_whole_parcellated_cleaned.shape,\n",
    "    df_conn_sim_parcellated_cleaned.shape, df_conn_sim_parcellated_pruned.shape\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save our work\n",
    "    uncomment this, or build a block like it if saving is necessary.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "df_expr_whole_parcellated_pruned.to_csv(\"./ahba_expr_whole_parby-glasser.tsv\", sep=\"\\t\")\n",
    "df_expr_whole_parcellated_pruned.to_pickle(\"./ahba_expr_whole_parby-glasser.df\")\n",
    "df_conn_parcellated_pruned.to_csv(\"./hcp_conn_whole_parby-glasser.tsv\", sep=\"\\t\")\n",
    "df_conn_parcellated_pruned.to_pickle(\"./hcp_conn_whole_parby-glasser.df\")\n",
    "df_connsim_parcellated_pruned.to_csv(\"./hcp_conn_sim_whole_parby-glasser.tsv\", sep=\"\\t\")\n",
    "df_connsim_parcellated_pruned.to_pickle(\"./hcp_conn_sim_whole_parby-glasser.df\")\n",
    "\n",
    "df_expr_parcellated_pruned.to_pickle(\"/data/cache/glasserparcel-expression.df\")\n",
    "df_connsim_parcellated_pruned.to_pickle(\"/data/conn/hcp_niftismooth_grandmean_glasser_sim.df\")\n",
    "\"\"\"\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad infs, 0 bad nans\n",
      "0 bad infs, 0 bad nans\n"
     ]
    }
   ],
   "source": [
    "for df in [df_conn_parcellated_pruned, df_conn_sim_parcellated_pruned, ]:\n",
    "    print(\"{} bad infs, {} bad nans\".format(sum(sum(np.isinf(df.values))), sum(sum(np.isnan(df.values)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schmidt_geconn",
   "language": "python",
   "name": "schmidt_geconn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
